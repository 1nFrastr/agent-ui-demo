#!/usr/bin/env python3
"""
LLMè¿é€šæ€§æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. OpenAI APIè¿æ¥æ€§
2. LangChain LLMæœåŠ¡åˆå§‹åŒ–
3. æµå¼å“åº”åŠŸèƒ½
4. é”™è¯¯å¤„ç†æœºåˆ¶
5. å®Œæ•´çš„åˆ†ææµç¨‹

ä½¿ç”¨æ–¹æ³•ï¼š
uv run python scripts/test_llm_connectivity.py
"""

import asyncio
import os
import sys
import time
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.config import settings
from app.services.llm_service import get_llm_service, LLMService
from app.core.exceptions import AgentExecutionError


class LLMConnectivityTester:
    """LLMè¿é€šæ€§æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
        self.total_tests = 0
        self.passed_tests = 0
    
    def log_test(self, test_name: str, passed: bool, message: str = "", details: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
            status = "âœ… PASS"
        else:
            status = "âŒ FAIL"
        
        result = f"{status} | {test_name}"
        if message:
            result += f" | {message}"
        
        print(result)
        if details:
            print(f"     Details: {details}")
        
        self.test_results.append({
            "name": test_name,
            "passed": passed,
            "message": message,
            "details": details
        })
    
    def test_environment_setup(self):
        """æµ‹è¯•ç¯å¢ƒé…ç½®"""
        print("\nğŸ”§ Testing Environment Setup...")
        
        # æµ‹è¯•OpenAI APIå¯†é’¥
        api_key = settings.openai_api_key
        if api_key and api_key != "your_openai_api_key_here" and api_key != "test-key-placeholder":
            self.log_test("OpenAI API Key", True, f"Key configured (length: {len(api_key)})")
        else:
            self.log_test("OpenAI API Key", False, "API key not configured or using placeholder")
        
        # æµ‹è¯•é…ç½®å‚æ•°
        self.log_test("OpenAI Model", True, f"Model: {settings.openai_model}")
        self.log_test("OpenAI Base URL", True, f"URL: {settings.openai_base_url}")
        
        # æµ‹è¯•Tavilyé…ç½®ï¼ˆå¯é€‰ï¼‰
        tavily_key = settings.tavily_api_key
        if tavily_key and tavily_key != "your_tavily_api_key_here":
            self.log_test("Tavily API Key", True, "Tavily key configured")
        else:
            self.log_test("Tavily API Key", False, "Tavily key not configured (optional)")
    
    async def test_llm_service_initialization(self):
        """æµ‹è¯•LLMæœåŠ¡åˆå§‹åŒ–"""
        print("\nğŸš€ Testing LLM Service Initialization...")
        
        try:
            llm_service = get_llm_service()
            self.log_test("LLM Service Creation", True, "Service instance created successfully")
            
            # æ£€æŸ¥LLMå®ä¾‹
            if hasattr(llm_service, 'llm') and llm_service.llm:
                self.log_test("LangChain LLM Instance", True, "ChatOpenAI instance created")
            else:
                self.log_test("LangChain LLM Instance", False, "LLM instance not found")
            
            return llm_service
            
        except AgentExecutionError as e:
            if "API key" in str(e):
                self.log_test("LLM Service Creation", False, "API key configuration error", str(e))
            else:
                self.log_test("LLM Service Creation", False, "Agent execution error", str(e))
            return None
        except Exception as e:
            self.log_test("LLM Service Creation", False, "Unexpected error", str(e))
            return None
    
    async def test_simple_llm_call(self, llm_service: LLMService):
        """æµ‹è¯•ç®€å•çš„LLMè°ƒç”¨"""
        print("\nğŸ’¬ Testing Simple LLM Call...")
        
        if not llm_service:
            self.log_test("Simple LLM Call", False, "LLM service not available")
            return
        
        try:
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            mock_search_results = self.create_mock_search_results()
            mock_web_contents = self.create_mock_web_contents()
            
            # æµ‹è¯•æµå¼å“åº”
            start_time = time.time()
            chunks = []
            chunk_count = 0
            
            async for chunk in llm_service.generate_analysis_stream(
                "æµ‹è¯•æŸ¥è¯¢ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", 
                mock_search_results, 
                mock_web_contents, 
                "test_session"
            ):
                chunks.append(chunk)
                chunk_count += 1
                # if chunk_count <= 3:  # åªæ‰“å°å‰3ä¸ªå—
                print(f"     Received chunk {chunk_count}: '{chunk[:50]}...'")
                
                # é˜²æ­¢æµ‹è¯•æ—¶é—´è¿‡é•¿
                if time.time() - start_time > 30:
                    break
            
            duration = time.time() - start_time
            total_content = "".join(chunks)
            
            self.log_test("Streaming Response", True, 
                         f"Received {chunk_count} chunks, {len(total_content)} chars in {duration:.2f}s")
            
            if len(total_content) > 100:
                self.log_test("Content Quality", True, "Generated substantial content")
            else:
                self.log_test("Content Quality", False, "Generated content too short")
                
        except asyncio.TimeoutError:
            self.log_test("Simple LLM Call", False, "Request timeout")
        except Exception as e:
            self.log_test("Simple LLM Call", False, "API call failed", str(e))
    
    async def test_error_handling(self, llm_service: LLMService):
        """æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶"""
        print("\nğŸ›¡ï¸  Testing Error Handling...")
        
        if not llm_service:
            self.log_test("Error Handling", False, "LLM service not available")
            return
        
        try:
            # æµ‹è¯•ç©ºæŸ¥è¯¢
            chunks = []
            async for chunk in llm_service.generate_analysis_stream(
                "", self.create_mock_search_results(), [], "test_session"
            ):
                chunks.append(chunk)
            
            if chunks:
                self.log_test("Empty Query Handling", True, "Handled empty query gracefully")
            else:
                self.log_test("Empty Query Handling", False, "No response for empty query")
                
        except Exception as e:
            self.log_test("Empty Query Handling", True, "Error handled gracefully", str(e))
    
    def create_mock_search_results(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæœç´¢ç»“æœ"""
        class MockResult:
            def __init__(self, title, url, summary, domain):
                self.title = title
                self.url = url
                self.summary = summary
                self.domain = domain
        
        class MockSearchResults:
            def __init__(self):
                self.results = [
                    MockResult(
                        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                        "https://example.com/ai-intro",
                        "äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
                        "example.com"
                    ),
                    MockResult(
                        "äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸ",
                        "https://example.com/ai-applications",
                        "AIåœ¨åŒ»ç–—ã€é‡‘èã€æ•™è‚²ã€äº¤é€šç­‰å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚",
                        "example.com"
                    )
                ]
        
        return MockSearchResults()
    
    def create_mock_web_contents(self):
        """åˆ›å»ºæ¨¡æ‹Ÿç½‘é¡µå†…å®¹"""
        class MockMetadata:
            def __init__(self):
                self.author = "AIç ”ç©¶å›¢é˜Ÿ"
                self.publish_date = "2024-01-01"
                self.description = "å…³äºäººå·¥æ™ºèƒ½çš„ä»‹ç»æ€§æ–‡ç« "
                self.keywords = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
        
        class MockContent:
            def __init__(self):
                self.status = "success"
                self.title = "äººå·¥æ™ºèƒ½å®Œå…¨æŒ‡å—"
                self.url = "https://example.com/ai-guide"
                self.summary = "è¿™æ˜¯ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½åŸºç¡€æ¦‚å¿µå’Œåº”ç”¨çš„ç»¼åˆæ€§æ–‡ç« ã€‚"
                self.content = """
                äººå·¥æ™ºèƒ½(Artificial Intelligence, AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯ï¼Œ
                å®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
                
                AIçš„ä¸»è¦ç ”ç©¶é¢†åŸŸåŒ…æ‹¬ï¼š
                1. æœºå™¨å­¦ä¹ (Machine Learning)
                2. æ·±åº¦å­¦ä¹ (Deep Learning)  
                3. è‡ªç„¶è¯­è¨€å¤„ç†(NLP)
                4. è®¡ç®—æœºè§†è§‰(Computer Vision)
                5. æœºå™¨äººå­¦(Robotics)
                
                è¿™äº›æŠ€æœ¯æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚
                """
                self.metadata = MockMetadata()
                self.images = []
        
        return [MockContent()]
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ§ª LLM Connectivity Test Suite")
        print("=" * 50)
        
        # ç¯å¢ƒé…ç½®æµ‹è¯•
        self.test_environment_setup()
        
        # LLMæœåŠ¡åˆå§‹åŒ–æµ‹è¯•
        llm_service = await self.test_llm_service_initialization()
        
        # å¦‚æœæœ‰æœ‰æ•ˆçš„APIå¯†é’¥ï¼Œè¿›è¡Œå®é™…çš„LLMè°ƒç”¨æµ‹è¯•
        if llm_service and settings.openai_api_key and settings.openai_api_key not in [
            "your_openai_api_key_here", "test-key-placeholder", "sk-test-placeholder"
        ]:
            await self.test_simple_llm_call(llm_service)
            await self.test_error_handling(llm_service)
        else:
            print("\nâš ï¸  Skipping LLM API tests - no valid API key configured")
            self.log_test("API Call Tests", False, "Skipped - no valid API key")
        
        # æµ‹è¯•ç»“æœæ±‡æ€»
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•ç»“æœæ±‡æ€»"""
        print("\n" + "=" * 50)
        print("ğŸ“Š Test Results Summary")
        print("=" * 50)
        
        success_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        
        print(f"Total Tests: {self.total_tests}")
        print(f"Passed: {self.passed_tests}")
        print(f"Failed: {self.total_tests - self.passed_tests}")
        print(f"Success Rate: {success_rate:.1f}%")
        
        if success_rate >= 80:
            print("\nğŸ‰ LLM connectivity looks good!")
        elif success_rate >= 60:
            print("\nâš ï¸  LLM connectivity has some issues")
        else:
            print("\nâŒ LLM connectivity has major issues")
        
        print("\nğŸ“ Configuration Tips:")
        if not settings.openai_api_key or settings.openai_api_key in [
            "your_openai_api_key_here", "test-key-placeholder"
        ]:
            print("- Set OPENAI_API_KEY in your .env file")
        
        print("- Ensure you have sufficient OpenAI API credits")
        print("- Check your internet connection")
        print("- Verify firewall settings allow OpenAI API access")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        tester = LLMConnectivityTester()
        await tester.run_all_tests()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Test interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Test suite failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())