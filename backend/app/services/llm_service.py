"""LLM service with LangChain integration and streaming support."""

import logging
from typing import AsyncGenerator, Dict, Any, Optional
import asyncio

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import AsyncCallbackHandler

from app.config import settings
from app.core.exceptions import AgentExecutionError


logger = logging.getLogger(__name__)


class StreamingCallbackHandler(AsyncCallbackHandler):
    """Custom callback handler for streaming LLM responses."""
    
    def __init__(self):
        self.content_queue = asyncio.Queue()
        self.finished = False
    
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Handle new token from LLM."""
        await self.content_queue.put(token)
    
    async def on_llm_end(self, response, **kwargs) -> None:
        """Handle LLM completion."""
        self.finished = True
        await self.content_queue.put(None)  # Signal completion


class LLMService:
    """LLM service with streaming support using LangChain."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        if not settings.openai_api_key:
            raise AgentExecutionError(
                "OpenAI API key is not configured",
                details={"config_error": "Missing OPENAI_API_KEY"}
            )
        
        # Initialize LangChain ChatOpenAI
        self.llm = ChatOpenAI(
            model=settings.openai_model,
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url,
            streaming=True,
            temperature=0.7,
            max_tokens=4000,
        )
    
    async def generate_analysis_stream(
        self,
        query: str,
        search_results: Any,
        web_contents: list,
        session_id: str
    ) -> AsyncGenerator[str, None]:
        """Generate streaming analysis report using LLM."""
        
        try:
            # Prepare context for LLM
            context = self._prepare_analysis_context(query, search_results, web_contents)
            
            # Create system and user messages
            system_message = SystemMessage(content=self._get_analysis_system_prompt())
            user_message = HumanMessage(content=context)
            
            # Create callback handler for streaming
            callback_handler = StreamingCallbackHandler()
            
            # Start LLM generation in background
            llm_task = asyncio.create_task(
                self._generate_with_callback(
                    [system_message, user_message], 
                    callback_handler
                )
            )
            
            # Stream tokens as they arrive
            while True:
                try:
                    # Wait for next token with timeout
                    token = await asyncio.wait_for(
                        callback_handler.content_queue.get(), 
                        timeout=30.0
                    )
                    
                    if token is None:  # Completion signal
                        break
                    
                    yield token
                    
                except asyncio.TimeoutError:
                    self.logger.error("LLM streaming timeout")
                    break
            
            # Ensure the LLM task completes
            await llm_task
            
        except Exception as e:
            self.logger.error(f"LLM analysis failed: {e}", exc_info=True)
            # Yield error message to client
            yield f"\n\nâš ï¸ **åˆ†æžç”Ÿæˆå¤±è´¥**: {str(e)}\n\nè¯·ç¨åŽé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
    
    async def _generate_with_callback(self, messages, callback_handler):
        """Generate LLM response with callback handler."""
        try:
            await self.llm.agenerate([messages], callbacks=[callback_handler])
        except Exception as e:
            self.logger.error(f"LLM generation error: {e}")
            await callback_handler.content_queue.put(None)
    
    def _prepare_analysis_context(self, query: str, search_results: Any, web_contents: list) -> str:
        """Prepare context information for LLM analysis."""
        context = f"""è¯·åŸºäºŽä»¥ä¸‹ä¿¡æ¯ä¸ºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆè¯¦ç»†çš„ç ”ç©¶åˆ†æžæŠ¥å‘Šã€‚

## ç”¨æˆ·æŸ¥è¯¢
{query}

## æœç´¢ç»“æžœ ({len(search_results.results)} ä¸ªç»“æžœ)
"""
        
        # Add search results
        for i, result in enumerate(search_results.results[:5], 1):
            context += f"""
### ç»“æžœ {i}: {result.title}
- **URL**: {result.url}
- **æ‘˜è¦**: {result.summary}
- **æ¥æº**: {result.domain}
"""
        
        # Add web content details
        context += f"\n## è¯¦ç»†å†…å®¹åˆ†æž ({len(web_contents)} ä¸ªé¡µé¢)\n"
        
        success_contents = [c for c in web_contents if c.status == "success"]
        for i, content in enumerate(success_contents[:3], 1):
            context += f"""
### å†…å®¹æº {i}: {content.title}
- **URL**: {content.url}
- **çŠ¶æ€**: {content.status}
"""
            
            if content.summary:
                context += f"- **æ‘˜è¦**: {content.summary}\n"
            
            if content.metadata:
                if content.metadata.author:
                    context += f"- **ä½œè€…**: {content.metadata.author}\n"
                if content.metadata.publish_date:
                    context += f"- **å‘å¸ƒæ—¶é—´**: {content.metadata.publish_date}\n"
                if content.metadata.description:
                    context += f"- **æè¿°**: {content.metadata.description}\n"
            
            # Add content excerpt (limit to avoid token overflow)
            if content.content:
                excerpt = content.content[:1500] if len(content.content) > 1500 else content.content
                context += f"- **å†…å®¹èŠ‚é€‰**: {excerpt}...\n"
        
        return context
    
    def _get_analysis_system_prompt(self) -> str:
        """Get system prompt for analysis generation."""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æžå¸ˆï¼Œæ“…é•¿ä»Žå¤šä¸ªä¿¡æ¯æºä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç»¼åˆæ€§åˆ†æžæŠ¥å‘Šã€‚

è¯·åŸºäºŽæä¾›çš„æœç´¢ç»“æžœå’Œç½‘é¡µå†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç»“æž„åŒ–çš„ç ”ç©¶æŠ¥å‘Šã€‚æŠ¥å‘Šåº”è¯¥åŒ…æ‹¬ï¼š

1. **ðŸ“‹ æ‰§è¡Œæ‘˜è¦** - ç®€æ´æ¦‚æ‹¬ä¸»è¦å‘çŽ°
2. **ðŸ” ä¿¡æ¯æºåˆ†æž** - è¯„ä¼°å„ä¿¡æ¯æºçš„å¯é æ€§å’Œç›¸å…³æ€§  
3. **ðŸ“Š æ ¸å¿ƒå‘çŽ°** - åŸºäºŽè¯æ®çš„å…³é”®å‘çŽ°ï¼Œå¼•ç”¨å…·ä½“æ¥æº
4. **ðŸ”— äº¤å‰éªŒè¯** - ä¸åŒæ¥æºé—´ä¿¡æ¯çš„ä¸€è‡´æ€§å’Œå·®å¼‚
5. **ðŸ’¡ æ·±åº¦æ´žå¯Ÿ** - åŸºäºŽæ•°æ®çš„åˆ†æžå’ŒæŽ¨è®º
6. **ðŸ“– ç»“è®ºä¸Žå»ºè®®** - ç»¼åˆæ€§ç»“è®ºå’Œè¿›ä¸€æ­¥ç ”ç©¶å»ºè®®
7. **ðŸ“š å‚è€ƒèµ„æ–™** - åˆ—å‡ºæ‰€æœ‰ä¿¡æ¯æ¥æº

è¦æ±‚ï¼š
- ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æž„æ¸…æ™°
- åŸºäºŽäº‹å®žï¼Œé¿å…ä¸»è§‚æŽ¨æµ‹
- å¼•ç”¨å…·ä½“æ¥æºæ”¯æŒè®ºç‚¹
- è¯­è¨€ä¸“ä¸šä½†æ˜“äºŽç†è§£
- å¦‚æžœä¿¡æ¯ä¸è¶³æˆ–æœ‰çŸ›ç›¾ï¼Œè¦æ˜Žç¡®æŒ‡å‡º
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„åˆ†æžæ€åº¦

è¯·ç¡®ä¿åˆ†æžçš„æ·±åº¦å’Œå¹¿åº¦éƒ½èƒ½æ»¡è¶³ä¸“ä¸šç ”ç©¶çš„æ ‡å‡†ã€‚"""


# Global LLM service instance  
_llm_service: Optional[LLMService] = None


def get_llm_service() -> LLMService:
    """Get or create LLM service instance."""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service