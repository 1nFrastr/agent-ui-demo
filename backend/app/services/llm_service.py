"""LLM service with LangChain integration and streaming support."""

import logging
import os
from typing import AsyncGenerator, Dict, Any, Optional
import asyncio

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langsmith import Client
from langsmith.evaluation import evaluate, LangChainStringEvaluator

from app.config import settings
from app.core.exceptions import AgentExecutionError


logger = logging.getLogger(__name__)


class LLMService:
    """LLM service with streaming support using LangChain."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # è®°å½•åˆå§‹åŒ–å¼€å§‹æ—¶é—´
        init_start = asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else 0
        
        # Configure LangSmith tracing if enabled
        self.logger.info("ðŸ”§ Configuring LangSmith tracing...")
        self._configure_langsmith_tracing()
        
        if not settings.openai_api_key:
            raise AgentExecutionError(
                "OpenAI API key is not configured",
                details={"config_error": "Missing OPENAI_API_KEY"}
            )
        
        # Initialize LangChain ChatOpenAI
        self.logger.info("ðŸ¤– Initializing ChatOpenAI client...")
        self.llm = ChatOpenAI(
            model=settings.openai_model,
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url,
            streaming=True,
            temperature=0.7,
            max_tokens=4000,
        )
        
        # Initialize LangSmith client for additional monitoring
        self.logger.info("ðŸ“Š Initializing LangSmith client...")
        self.langsmith_client = self._init_langsmith_client()
        
        # è®°å½•åˆå§‹åŒ–å®Œæˆæ—¶é—´
        if init_start > 0:
            duration = asyncio.get_event_loop().time() - init_start
            self.logger.info(f"âœ… LLMService initialization completed in {duration:.2f}s")
        else:
            self.logger.info("âœ… LLMService initialization completed")
    
    def _configure_langsmith_tracing(self):
        """Configure LangSmith environment variables for tracing."""
        if settings.langsmith_tracing and settings.langsmith_api_key:
            # Set environment variables for LangSmith
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_API_KEY"] = settings.langsmith_api_key
            os.environ["LANGCHAIN_PROJECT"] = settings.langsmith_project
            os.environ["LANGCHAIN_ENDPOINT"] = settings.langsmith_endpoint
            
            self.logger.info(f"LangSmith tracing enabled for project: {settings.langsmith_project}")
        else:
            # Disable tracing if not configured
            os.environ["LANGCHAIN_TRACING_V2"] = "false"
            self.logger.info("LangSmith tracing disabled")
    
    def _init_langsmith_client(self) -> Optional[Client]:
        """Initialize LangSmith client for additional operations."""
        if settings.langsmith_tracing and settings.langsmith_api_key:
            try:
                client = Client(
                    api_key=settings.langsmith_api_key,
                    api_url=settings.langsmith_endpoint
                )
                self.logger.info("LangSmith client initialized successfully")
                return client
            except Exception as e:
                self.logger.warning(f"Failed to initialize LangSmith client: {e}")
                return None
        return None
    
    async def generate_analysis_stream(
        self,
        query: str,
        search_results: Any,
        web_contents: list,
        session_id: str
    ) -> AsyncGenerator[str, None]:
        """Generate streaming analysis report using LLM."""
        
        try:
            # Prepare context for LLM
            context = self._prepare_analysis_context(query, search_results, web_contents)
            
            # Create system and user messages
            system_message = SystemMessage(content=self._get_analysis_system_prompt())
            user_message = HumanMessage(content=context)
            
            # Add metadata for LangSmith tracing
            metadata = {
                "session_id": session_id,
                "query": query,
                "search_results_count": len(search_results.results) if hasattr(search_results, 'results') else 0,
                "web_contents_count": len(web_contents),
                "model": settings.openai_model,
                "operation": "analysis_generation"
            }
            
            # Set run name for better tracing
            run_name = f"analysis_generation_{session_id[:8]}"
            
            # ç›´æŽ¥ä½¿ç”¨LangChainçš„streamingæ”¯æŒï¼Œæ›´ç®€æ´çš„æ–¹å¼
            try:
                async for chunk in self.llm.astream([system_message, user_message]):
                    if chunk.content:  # åªyieldéžç©ºå†…å®¹
                        yield chunk.content
            except asyncio.TimeoutError:
                self.logger.error("LLM streaming timeout")
                yield "\n\nâš ï¸ **å“åº”è¶…æ—¶**: è¯·ç¨åŽé‡è¯•ã€‚"
            except Exception as stream_error:
                self.logger.error(f"LLM streaming error: {stream_error}")
                yield f"\n\nâš ï¸ **æµå¼å“åº”é”™è¯¯**: {str(stream_error)}"
            
        except Exception as e:
            self.logger.error(f"LLM analysis failed: {e}", exc_info=True)
            # Yield error message to client
            yield f"\n\nâš ï¸ **åˆ†æžç”Ÿæˆå¤±è´¥**: {str(e)}\n\nè¯·ç¨åŽé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
    
    def _prepare_analysis_context(self, query: str, search_results: Any, web_contents: list) -> str:
        """Prepare context information for LLM analysis."""
        context = f"""è¯·åŸºäºŽä»¥ä¸‹ä¿¡æ¯ä¸ºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆè¯¦ç»†çš„ç ”ç©¶åˆ†æžæŠ¥å‘Šã€‚

## ç”¨æˆ·æŸ¥è¯¢
{query}

## æœç´¢ç»“æžœ ({len(search_results.results)} ä¸ªç»“æžœ)
"""
        
        # Add search results
        for i, result in enumerate(search_results.results[:5], 1):
            context += f"""
### ç»“æžœ {i}: {result.title}
- **URL**: {result.url}
- **æ‘˜è¦**: {result.summary}
- **æ¥æº**: {result.domain}
"""
        
        # Add web content details
        context += f"\n## è¯¦ç»†å†…å®¹åˆ†æž ({len(web_contents)} ä¸ªé¡µé¢)\n"
        
        success_contents = [c for c in web_contents if c.status == "success"]
        for i, content in enumerate(success_contents[:3], 1):
            context += f"""
### å†…å®¹æº {i}: {content.title}
- **URL**: {content.url}
- **çŠ¶æ€**: {content.status}
"""
            
            if content.summary:
                context += f"- **æ‘˜è¦**: {content.summary}\n"
            
            if content.metadata:
                if content.metadata.author:
                    context += f"- **ä½œè€…**: {content.metadata.author}\n"
                if content.metadata.publishDate:
                    context += f"- **å‘å¸ƒæ—¶é—´**: {content.metadata.publishDate}\n"
                if content.metadata.description:
                    context += f"- **æè¿°**: {content.metadata.description}\n"
            
            # Add content excerpt (limit to avoid token overflow)
            if content.content:
                excerpt = content.content[:1500] if len(content.content) > 1500 else content.content
                context += f"- **å†…å®¹èŠ‚é€‰**: {excerpt}...\n"
        
        return context
    
    def _get_analysis_system_prompt(self) -> str:
        """Get system prompt for analysis generation."""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æžå¸ˆï¼Œæ“…é•¿ä»Žå¤šä¸ªä¿¡æ¯æºä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç»¼åˆæ€§åˆ†æžæŠ¥å‘Šã€‚

è¯·åŸºäºŽæä¾›çš„æœç´¢ç»“æžœå’Œç½‘é¡µå†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç»“æž„åŒ–çš„ç ”ç©¶æŠ¥å‘Šã€‚æŠ¥å‘Šåº”è¯¥åŒ…æ‹¬ï¼š

1. **ðŸ“‹ æ‰§è¡Œæ‘˜è¦** - ç®€æ´æ¦‚æ‹¬ä¸»è¦å‘çŽ°
2. **ðŸ” ä¿¡æ¯æºåˆ†æž** - è¯„ä¼°å„ä¿¡æ¯æºçš„å¯é æ€§å’Œç›¸å…³æ€§  
3. **ðŸ“Š æ ¸å¿ƒå‘çŽ°** - åŸºäºŽè¯æ®çš„å…³é”®å‘çŽ°ï¼Œå¼•ç”¨å…·ä½“æ¥æº
4. **ðŸ”— äº¤å‰éªŒè¯** - ä¸åŒæ¥æºé—´ä¿¡æ¯çš„ä¸€è‡´æ€§å’Œå·®å¼‚
5. **ðŸ’¡ æ·±åº¦æ´žå¯Ÿ** - åŸºäºŽæ•°æ®çš„åˆ†æžå’ŒæŽ¨è®º
6. **ðŸ“– ç»“è®ºä¸Žå»ºè®®** - ç»¼åˆæ€§ç»“è®ºå’Œè¿›ä¸€æ­¥ç ”ç©¶å»ºè®®
7. **ðŸ“š å‚è€ƒèµ„æ–™** - åˆ—å‡ºæ‰€æœ‰ä¿¡æ¯æ¥æº

è¦æ±‚ï¼š
- ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æž„æ¸…æ™°
- åŸºäºŽäº‹å®žï¼Œé¿å…ä¸»è§‚æŽ¨æµ‹
- å¼•ç”¨å…·ä½“æ¥æºæ”¯æŒè®ºç‚¹
- è¯­è¨€ä¸“ä¸šä½†æ˜“äºŽç†è§£
- å¦‚æžœä¿¡æ¯ä¸è¶³æˆ–æœ‰çŸ›ç›¾ï¼Œè¦æ˜Žç¡®æŒ‡å‡º
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„åˆ†æžæ€åº¦

è¯·ç¡®ä¿åˆ†æžçš„æ·±åº¦å’Œå¹¿åº¦éƒ½èƒ½æ»¡è¶³ä¸“ä¸šç ”ç©¶çš„æ ‡å‡†ã€‚"""
    
    async def generate_completion(
        self,
        prompt: str,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        session_id: str = None
    ) -> str:
        """Generate a single completion response."""
        try:
            # Create messages
            user_message = HumanMessage(content=prompt)
            
            # Add metadata for tracing
            metadata = {
                "session_id": session_id or "unknown",
                "model": settings.openai_model,
                "operation": "code_generation",
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            # Configure LLM parameters
            llm_with_params = self.llm.bind(
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            # Generate response
            # TODO: æ”¹æˆæµå¼ä¼ è¾“
            response = await llm_with_params.ainvoke([user_message])
            
            return response.content
            
        except Exception as e:
            self.logger.error(f"LLM completion failed: {e}", exc_info=True)
            raise
    
    def log_analysis_completion(self, session_id: str, query: str, success: bool, error_msg: str = None):
        """Log analysis completion to LangSmith for monitoring."""
        if self.langsmith_client:
            try:
                # Create a run record for the analysis task
                run_data = {
                    "name": f"analysis_task_{session_id[:8]}",
                    "inputs": {"query": query},
                    "run_type": "llm",
                    "session_id": session_id,
                }
                
                if success:
                    run_data["outputs"] = {"status": "success"}
                else:
                    run_data["outputs"] = {"status": "failed", "error": error_msg}
                
                # Note: This is a simplified logging approach
                # In production, you might want to use more sophisticated tracking
                self.logger.info(f"Analysis task logged to LangSmith: {run_data}")
                
            except Exception as e:
                self.logger.warning(f"Failed to log to LangSmith: {e}")


# Global LLM service instance  
_llm_service: Optional[LLMService] = None
_initialization_lock = asyncio.Lock()


def get_llm_service() -> LLMService:
    """Get or create LLM service instance with improved initialization."""
    global _llm_service
    
    if _llm_service is None:
        logger.info("ðŸ”„ Creating new LLM service instance...")
        start_time = asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else 0
        
        _llm_service = LLMService()
        
        if start_time > 0:
            duration = asyncio.get_event_loop().time() - start_time
            logger.info(f"âš¡ LLM service created in {duration:.2f}s")
        else:
            logger.info("âš¡ LLM service created successfully")
    
    return _llm_service


async def get_llm_service_async() -> LLMService:
    """Get or create LLM service instance with async initialization and locking."""
    global _llm_service
    
    if _llm_service is None:
        async with _initialization_lock:
            if _llm_service is None:  # Double-check pattern
                logger.info("ðŸ”„ Async creating new LLM service instance...")
                start_time = asyncio.get_event_loop().time()
                
                # Run the sync initialization in a thread pool
                loop = asyncio.get_event_loop()
                _llm_service = await loop.run_in_executor(None, LLMService)
                
                duration = asyncio.get_event_loop().time() - start_time
                logger.info(f"âš¡ LLM service async created in {duration:.2f}s")
    
    return _llm_service