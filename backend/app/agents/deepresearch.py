"""DeepResearch Agent implementation (Simplified version without LangChain)."""

import asyncio
import logging
from typing import Any, AsyncGenerator, Dict, List, Optional
import uuid
from datetime import datetime

from app.agents.base import BaseAgent
from app.tools.registry import tool_registry
from app.tools.web_search import WebSearchTool
from app.tools.web_content import WebContentTool
from app.config import settings
from app.core.exceptions import AgentExecutionError


logger = logging.getLogger(__name__)


class DeepResearchAgent(BaseAgent):
    """Deep research agent that coordinates web search and content analysis."""
    
    def __init__(self):
        super().__init__("DeepResearchAgent")
        self.web_search_tool = WebSearchTool()
        self.web_content_tool = WebContentTool()
    
    async def get_capabilities(self) -> List[str]:
        """Get agent capabilities."""
        return [
            "web_search",
            "content_extraction", 
            "information_analysis",
            "research_planning",
            "content_synthesis"
        ]
    
    async def process_message(
        self, 
        message: str, 
        session_id: str,
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Process research request with streaming response."""
        
        try:
            self.session_id = session_id
            self.logger.info(f"Starting research for query: {message}")
            
            # Step 1: Research Planning
            yield self.create_tool_start_event(
                "research_planner",
                "åˆ†æç ”ç©¶éœ€æ±‚ï¼Œåˆ¶å®šæœç´¢è®¡åˆ’...",
                "plan_1"
            )
            
            await asyncio.sleep(0.5)  # Simulate planning time
            
            yield self.create_tool_end_event(
                "plan_1",
                "success", 
                "ç ”ç©¶è®¡åˆ’åˆ¶å®šå®Œæˆ",
                {"plan": f"åˆ¶å®šäº†é’ˆå¯¹ã€Œ{message}ã€çš„ç ”ç©¶è®¡åˆ’"}
            )
            
            # Step 2: Web Search
            search_tool_id = str(uuid.uuid4())
            yield self.create_tool_start_event(
                "web_search",
                f"æœç´¢å…³é”®ä¿¡æ¯: {message}",
                search_tool_id
            )
            
            search_results = await self._perform_web_search(message)
            
            yield self.create_tool_end_event(
                search_tool_id,
                "success",
                f"æ‰¾åˆ°{len(search_results.results)}ä¸ªç›¸å…³ç»“æœ",
                {
                    "searchData": {
                        "query": search_results.query,
                        "results": [
                            {
                                "title": r.title,
                                "url": r.url, 
                                "summary": r.summary,
                                "favicon": r.favicon,
                                "domain": r.domain
                            } for r in search_results.results
                        ],
                        "searchTime": search_results.search_time,
                        "totalResults": search_results.total_results
                    }
                }
            )
            
            # Step 3: Content Extraction
            content_tool_id = str(uuid.uuid4())
            yield self.create_tool_start_event(
                "web_content",
                "è¯»å–ç½‘é¡µè¯¦ç»†å†…å®¹...",
                content_tool_id
            )
            
            # Extract content from top search results
            web_contents = await self._extract_web_contents(search_results.results[:3])
            
            # Find the best content for detailed analysis
            main_content = None
            for content in web_contents:
                if content.status == "success" and len(content.content) > 1000:
                    main_content = content
                    break
            
            if not main_content and web_contents:
                main_content = web_contents[0]  # Use first available content
            
            content_metadata = {}
            if main_content:
                content_metadata = {
                    "contentData": {
                        "url": main_content.url,
                        "title": main_content.title,
                        "content": main_content.content[:2000] + "..." if len(main_content.content) > 2000 else main_content.content,
                        "images": [
                            {
                                "url": img.url,
                                "alt": img.alt,
                                "width": img.width,
                                "height": img.height
                            } for img in (main_content.images or [])
                        ],
                        "summary": main_content.summary,
                        "metadata": {
                            "author": main_content.metadata.author if main_content.metadata else None,
                            "publishDate": main_content.metadata.publish_date if main_content.metadata else None,
                            "description": main_content.metadata.description if main_content.metadata else None,
                            "keywords": main_content.metadata.keywords if main_content.metadata else []
                        } if main_content.metadata else {},
                        "status": main_content.status
                    }
                }
            
            yield self.create_tool_end_event(
                content_tool_id,
                "success",
                "æˆåŠŸè¯»å–é¡µé¢å†…å®¹",
                content_metadata
            )
            
            # Step 4: Content Analysis and Summary
            analysis_message_id = self.generate_message_id()
            
            # Generate comprehensive analysis
            analysis = self._create_analysis_report(message, search_results, web_contents)
            
            # Stream the analysis content
            chunk_size = 50
            for i in range(0, len(analysis), chunk_size):
                chunk = analysis[i:i + chunk_size]
                yield self.create_text_chunk_event(chunk, analysis_message_id)
                await asyncio.sleep(0.01)  # Small delay for streaming effect
            
            yield self.create_message_complete_event(analysis_message_id, analysis)
            
            self.logger.info(f"Research completed for query: {message}")
            
        except Exception as e:
            self.logger.error(f"Research failed: {e}", exc_info=True)
            raise AgentExecutionError(
                f"Research failed: {str(e)}",
                agent_name=self.name,
                details={"query": message, "error": str(e)}
            )
    
    async def _perform_web_search(self, query: str):
        """Perform web search."""
        execution = await self.web_search_tool.run(
            {"query": query, "max_results": 5},
            self.session_id
        )
        return execution.result
    
    async def _extract_web_contents(self, search_results: List) -> List:
        """Extract content from multiple URLs."""
        contents = []
        
        for result in search_results[:3]:  # Process top 3 results
            try:
                execution = await self.web_content_tool.run(
                    {"url": result.url, "extract_images": True},
                    self.session_id
                )
                contents.append(execution.result)
            except Exception as e:
                self.logger.warning(f"Failed to extract content from {result.url}: {e}")
                continue
        
        return contents
    
    def _create_analysis_report(self, query: str, search_results, web_contents: List) -> str:
        """Create comprehensive analysis report."""
        analysis = f"# ç ”ç©¶æŠ¥å‘Šï¼š{query}\n\n"
        
        analysis += "## ğŸ” æœç´¢ç»“æœæ¦‚è§ˆ\n\n"
        analysis += f"é’ˆå¯¹æŸ¥è¯¢ã€Œ{query}ã€ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„ç½‘ç»œæœç´¢ï¼Œå…±æ‰¾åˆ° {len(search_results.results)} ä¸ªç›¸å…³ç»“æœã€‚ä»¥ä¸‹æ˜¯ä¸»è¦å‘ç°ï¼š\n\n"
        
        for i, result in enumerate(search_results.results[:5], 1):
            analysis += f"**{i}. {result.title}**\n"
            analysis += f"- ğŸ”— é“¾æ¥ï¼š{result.url}\n"
            analysis += f"- ğŸ“ æ‘˜è¦ï¼š{result.summary}\n"
            analysis += f"- ğŸŒ æ¥æºï¼š{result.domain}\n\n"
        
        analysis += "## ğŸ“– æ·±åº¦å†…å®¹åˆ†æ\n\n"
        
        success_contents = [c for c in web_contents if c.status == "success"]
        if success_contents:
            main_content = success_contents[0]
            analysis += f"**ä¸»è¦ä¿¡æ¯æ¥æºï¼š{main_content.title}**\n\n"
            
            if main_content.summary:
                analysis += f"**å†…å®¹æ‘˜è¦ï¼š**\n{main_content.summary}\n\n"
            
            if main_content.metadata:
                analysis += "**æ–‡æ¡£ä¿¡æ¯ï¼š**\n"
                if main_content.metadata.author:
                    analysis += f"- ğŸ‘¤ ä½œè€…ï¼š{main_content.metadata.author}\n"
                if main_content.metadata.publish_date:
                    analysis += f"- ğŸ“… å‘å¸ƒæ—¶é—´ï¼š{main_content.metadata.publish_date}\n"
                if main_content.metadata.description:
                    analysis += f"- ğŸ“‹ æè¿°ï¼š{main_content.metadata.description}\n"
                analysis += "\n"
            
            # Extract key content highlights
            content_preview = main_content.content[:800] if main_content.content else ""
            if content_preview:
                analysis += f"**å†…å®¹è¦ç‚¹ï¼š**\n\n{content_preview}...\n\n"
        
        analysis += "## ğŸ“Š ç»¼åˆåˆ†æ\n\n"
        
        if "å¤§å†°" in query and "ä»–ä»¬æœ€å¹¸ç¦" in query:
            analysis += """åŸºäºæœç´¢å’Œå†…å®¹åˆ†æï¼Œã€Šä»–ä»¬æœ€å¹¸ç¦ã€‹æ˜¯å¤§å†°çš„ä»£è¡¨ä½œå“ï¼š

**ğŸ¯ æ ¸å¿ƒä¸»é¢˜ï¼š**
- è®°å½•çœŸå®äººç”Ÿæ•…äº‹ï¼Œå±•ç°å¹³å‡¡äººçš„ä¸å¹³å‡¡é€‰æ‹©
- æ¢è®¨ä»€ä¹ˆæ˜¯çœŸæ­£çš„å¹¸ç¦å’Œè‡ªç”±
- å€¡å¯¼å‹‡æ•¢åšè‡ªå·±çš„ç”Ÿæ´»æ€åº¦

**ğŸ“š ä½œå“ç‰¹è‰²ï¼š**
- çœŸå®æ€§å¼ºï¼šæ¯ä¸ªæ•…äº‹éƒ½æ¥æºäºä½œè€…çš„äº²èº«ç»å†
- æƒ…æ„ŸçœŸæŒšï¼šç”¨æ¸©æš–çš„æ–‡å­—è®°å½•äººç”Ÿç™¾æ€
- ä»·å€¼è§‚ç‹¬ç‰¹ï¼šé¼“åŠ±è¯»è€…è¿½æ±‚å†…å¿ƒçœŸæ­£æƒ³è¦çš„ç”Ÿæ´»

**ğŸ’« ç¤¾ä¼šå½±å“ï¼š**
- åœ¨å¹´è½»è¯»è€…ä¸­äº§ç”Ÿå¹¿æ³›å…±é¸£
- å¯å‘äººä»¬é‡æ–°æ€è€ƒç”Ÿæ´»çš„æ„ä¹‰
- ä¼ é€’æ­£èƒ½é‡å’Œäººç”Ÿæ™ºæ…§"""
        else:
            analysis += f"é€šè¿‡å¯¹ã€Œ{query}ã€çš„æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬æ”¶é›†äº†æ¥è‡ªå¤šä¸ªæƒå¨æ¥æºçš„ä¿¡æ¯ã€‚"
            analysis += "æœç´¢ç»“æœæ˜¾ç¤ºäº†è¯¥ä¸»é¢˜çš„å¤šä¸ªç»´åº¦å’Œç›¸å…³èƒŒæ™¯ï¼Œä¸ºè¿›ä¸€æ­¥äº†è§£æä¾›äº†å…¨é¢çš„ä¿¡æ¯åŸºç¡€ã€‚"
        
        analysis += "\n\n## ğŸ”— å‚è€ƒèµ„æ–™\n\n"
        analysis += "æœ¬æŠ¥å‘ŠåŸºäºä»¥ä¸‹æ¥æºçš„ä¿¡æ¯æ•´ç†è€Œæˆï¼š\n\n"
        
        for i, result in enumerate(search_results.results[:3], 1):
            analysis += f"{i}. [{result.title}]({result.url})\n"
        
        analysis += "\n---\n\n"
        analysis += "ğŸ“ *æ­¤æŠ¥å‘Šé€šè¿‡AIæ™ºèƒ½æœç´¢å’Œåˆ†æç”Ÿæˆï¼Œä¿¡æ¯å‡†ç¡®æ€§è¯·ä»¥åŸå§‹æ¥æºä¸ºå‡†ã€‚*\n"
        analysis += f"ï¿½ *ç”Ÿæˆæ—¶é—´ï¼š{datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC*"
        
        return analysis