"""DeepResearch Agent implementation (Simplified version without LangChain)."""

import asyncio
import logging
from typing import Any, AsyncGenerator, Dict, List, Optional
import uuid
from datetime import datetime
import httpx

# LangSmith è¿½è¸ª
from langsmith import traceable

from app.agents.base import BaseAgent
from app.tools.registry import tool_registry
from app.tools.web_search import WebSearchTool
from app.tools.web_content import WebContentTool
from app.services.llm_service import get_llm_service
from app.config import settings
from app.core.exceptions import AgentExecutionError


logger = logging.getLogger(__name__)


class DeepResearchAgent(BaseAgent):
    """Deep research agent that coordinates web search and content analysis."""
    
    def __init__(self):
        super().__init__("DeepResearchAgent")
        self.web_search_tool = WebSearchTool()
        self.web_content_tool = WebContentTool()
        # ä½¿ç”¨åŒæ­¥æ–¹å¼åˆå§‹åŒ–ï¼Œåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
        self.llm_service = get_llm_service()
        # åˆ›å»ºå…±äº«çš„HTTPå®¢æˆ·ç«¯ç”¨äºçœŸæ­£çš„å¹¶è¡Œå¤„ç†
        self._shared_client = None
    
    async def _get_shared_client(self) -> httpx.AsyncClient:
        """è·å–æˆ–åˆ›å»ºå…±äº«çš„HTTPå®¢æˆ·ç«¯"""
        if self._shared_client is None or self._shared_client.is_closed:
            self._shared_client = httpx.AsyncClient(
                timeout=settings.request_timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
                limits=httpx.Limits(max_keepalive_connections=20, max_connections=100)
            )
        return self._shared_client
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._get_shared_client()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ï¼Œæ¸…ç†èµ„æº"""
        if self._shared_client and not self._shared_client.is_closed:
            await self._shared_client.aclose()
    
    async def get_capabilities(self) -> List[str]:
        """Get agent capabilities."""
        return [
            "web_search",
            "content_extraction", 
            "information_analysis",
            "research_planning",
            "content_synthesis"
        ]
    
    @traceable(name="deep_research_agent")
    async def process_message(
        self, 
        message: str, 
        session_id: str,
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Process research request with streaming response."""
        
        try:
            self.session_id = session_id
            self.logger.info(f"Starting research for query: {message}")
            
            # Step 1: Research Planning
            yield self.create_tool_start_event(
                "research_planner",
                "åˆ†æç ”ç©¶éœ€æ±‚ï¼Œåˆ¶å®šæœç´¢è®¡åˆ’...",
                "plan_1"
            )
            
            await asyncio.sleep(0.5)  # Simulate planning time
            
            yield self.create_tool_end_event(
                "plan_1",
                "success", 
                "ç ”ç©¶è®¡åˆ’åˆ¶å®šå®Œæˆ",
                {"plan": f"åˆ¶å®šäº†é’ˆå¯¹ã€Œ{message}ã€çš„ç ”ç©¶è®¡åˆ’"}
            )
            
            # æ‰¿ä¸Šå¯ä¸‹çš„è¯´æ˜æ–‡å­—
            planning_message_id = self.generate_message_id()
            yield self.create_text_chunk_event(
                f"âœ… æˆ‘å·²å®Œæˆå¯¹ã€Œ{message}ã€çš„ç ”ç©¶è®¡åˆ’åˆ¶å®šã€‚ç°åœ¨è®©æˆ‘å¼€å§‹è¿›è¡Œç½‘ç»œæœç´¢ï¼Œå¯»æ‰¾ç›¸å…³ä¿¡æ¯...\n\n",
                planning_message_id
            )
            
            # Step 2: Web Search
            search_tool_id = str(uuid.uuid4())
            yield self.create_tool_start_event(
                "web_search",
                f"æœç´¢å…³é”®ä¿¡æ¯: {message}",
                search_tool_id
            )
            
            search_results = await self._perform_web_search(message)
            
            yield self.create_tool_end_event(
                search_tool_id,
                "success",
                f"æ‰¾åˆ°{len(search_results.results)}ä¸ªç›¸å…³ç»“æœ",
                {
                    "searchData": {
                        "query": search_results.query,
                        "results": [
                            {
                                "title": r.title,
                                "url": r.url, 
                                "summary": r.summary,
                                "favicon": r.favicon,
                                "domain": r.domain
                            } for r in search_results.results
                        ],
                        "searchTime": search_results.searchTime,
                        "totalResults": search_results.totalResults
                    }
                }
            )
            
            # æ‰¿ä¸Šå¯ä¸‹çš„è¯´æ˜æ–‡å­—
            search_message_id = self.generate_message_id()
            yield self.create_text_chunk_event(
                f"ğŸ” æˆ‘å·²å®Œæˆç½‘ç»œæœç´¢ï¼Œæ‰¾åˆ°äº† {len(search_results.results)} ä¸ªç›¸å…³ç»“æœã€‚ç°åœ¨è®©æˆ‘è¯»å–è¿™äº›ç½‘é¡µçš„è¯¦ç»†å†…å®¹ï¼Œä»¥è·å–æ›´æ·±å…¥çš„ä¿¡æ¯...\n\n",
                search_message_id
            )
            
            # Step 3: Content Extraction
            content_tool_id = str(uuid.uuid4())
            yield self.create_tool_start_event(
                "web_content",
                "è¯»å–ç½‘é¡µè¯¦ç»†å†…å®¹...",
                content_tool_id
            )
            
            # Extract content from top search results
            web_contents = await self._extract_web_contents(search_results.results[:3])
            
            # Find the best content for detailed analysis
            main_content = None
            for content in web_contents:
                if content.status == "success" and len(content.content) > 1000:
                    main_content = content
                    break
            
            if not main_content and web_contents:
                main_content = web_contents[0]  # Use first available content
            
            content_metadata = {}
            if main_content:
                content_metadata = {
                    "contentData": {
                        "url": main_content.url,
                        "title": main_content.title,
                        "content": main_content.content[:2000] + "..." if len(main_content.content) > 2000 else main_content.content,
                        "images": [
                            {
                                "url": img.url,
                                "alt": img.alt,
                                "width": img.width,
                                "height": img.height
                            } for img in (main_content.images or [])
                        ],
                        "summary": main_content.summary,
                        "metadata": {
                            "author": main_content.metadata.author if main_content.metadata else None,
                            "publishDate": main_content.metadata.publishDate if main_content.metadata else None,
                            "description": main_content.metadata.description if main_content.metadata else None,
                            "keywords": main_content.metadata.keywords if main_content.metadata else []
                        } if main_content.metadata else {},
                        "status": main_content.status
                    }
                }
            
            yield self.create_tool_end_event(
                content_tool_id,
                "success",
                "æˆåŠŸè¯»å–é¡µé¢å†…å®¹",
                content_metadata
            )
            
            # æ‰¿ä¸Šå¯ä¸‹çš„è¯´æ˜æ–‡å­—
            content_message_id = self.generate_message_id()
            yield self.create_text_chunk_event(
                f"ğŸ“„ æˆ‘å·²æˆåŠŸè¯»å–ç½‘é¡µå†…å®¹ï¼Œè·å¾—äº†ä¸°å¯Œçš„è¯¦ç»†ä¿¡æ¯ã€‚ç°åœ¨è®©æˆ‘åŸºäºè¿™äº›æœç´¢ç»“æœå’Œç½‘é¡µå†…å®¹è¿›è¡ŒAIæ™ºèƒ½åˆ†æï¼Œä¸ºæ‚¨æä¾›æ·±åº¦æ´å¯Ÿ...\n\n",
                content_message_id
            )
            
            # Step 4: LLM-Powered Analysis with Streaming
            analysis_message_id = self.generate_message_id()
            
            # Start LLM analysis tool
            llm_tool_id = str(uuid.uuid4())
            yield self.create_tool_start_event(
                "llm_analysis",
                "åŸºäºæœç´¢ç»“æœè¿›è¡ŒAIæ™ºèƒ½åˆ†æ...",
                llm_tool_id
            )
            
            # Stream LLM analysis in real-time
            analysis_chunks = []
            async for chunk in self.llm_service.generate_analysis_stream(
                message, search_results, web_contents, session_id
            ):
                analysis_chunks.append(chunk)
                yield self.create_text_chunk_event(chunk, analysis_message_id)
            
            # Complete the analysis
            full_analysis = "".join(analysis_chunks)
            
            yield self.create_tool_end_event(
                llm_tool_id,
                "success",
                "AIåˆ†æå®Œæˆ",
                {
                    "analysis_length": len(full_analysis),
                    "model_used": settings.openai_model,
                    "tokens_estimated": len(full_analysis) // 4  # Rough estimate
                }
            )
            
            yield self.create_message_complete_event(analysis_message_id, full_analysis)
            
            # å‘é€æµç»“æŸäº‹ä»¶
            yield {
                "type": "session_end",
                "data": {
                    "sessionId": session_id,
                    "message": "å¯¹è¯æµå¤„ç†å®Œæˆ"
                }
            }
            
            self.logger.info(f"Research completed for query: {message}")
            
        except Exception as e:
            self.logger.error(f"Research failed: {e}", exc_info=True)
            raise AgentExecutionError(
                f"Research failed: {str(e)}",
                agent_name=self.name,
                details={"query": message, "error": str(e)}
            )
    
    @traceable(name="web_search")
    async def _perform_web_search(self, query: str):
        """Perform web search."""
        execution = await self.web_search_tool.run(
            {"query": query, "max_results": 5},
            self.session_id
        )
        return execution.result
    
    @traceable(name="content_extraction")
    async def _extract_web_contents(self, search_results: List) -> List:
        """Extract content from multiple URLs with true parallelization."""
        contents = []
        
        # è·å–å…±äº«å®¢æˆ·ç«¯
        client = await self._get_shared_client()
        
        # Create tasks for true parallel execution using create_task
        tasks = []
        for i, result in enumerate(search_results[:3]):  # Process top 3 results
            task = asyncio.create_task(self._extract_single_content_parallel(result, client, i+1))
            tasks.append(task)
        
        self.logger.info(f"ğŸ“‹ åˆ›å»ºäº†{len(tasks)}ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œå¼€å§‹åŒæ—¶æ‰§è¡Œ...")
        
        # Execute all tasks concurrently with true parallelization
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Process results and filter out exceptions
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.warning(f"Failed to extract content from {search_results[i].url}: {result}")
                elif result is not None:
                    contents.append(result)
        
        return contents
    
    @traceable(name="extract_single_url")
    async def _extract_single_content_parallel(self, search_result, client: httpx.AsyncClient, index: int):
        """ä½¿ç”¨å…±äº«å®¢æˆ·ç«¯è¿›è¡ŒçœŸæ­£çš„å¹¶è¡Œå†…å®¹æå–"""
        start_time = asyncio.get_event_loop().time()
        self.logger.info(f"[çœŸå¹¶è¡Œ] ç«‹å³å¼€å§‹å¤„ç†URL {index}: {search_result.url} (å¯åŠ¨æ—¶é—´: {start_time:.3f})")
        
        try:
            # ç›´æ¥ä½¿ç”¨å…±äº«å®¢æˆ·ç«¯è¿›è¡ŒHTTPè¯·æ±‚
            response = await client.get(search_result.url)
            response.raise_for_status()
            content = response.text
            
            # ç®€åŒ–çš„å†…å®¹å¤„ç†ï¼ˆé¿å…é¢å¤–çš„BeautifulSoupå¼€é”€ï¼‰
            from bs4 import BeautifulSoup
            
            # å¿«é€Ÿè§£æ
            try:
                soup = BeautifulSoup(content, 'lxml')
            except:
                soup = BeautifulSoup(content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_tag = soup.find('title')
            title = title_tag.get_text().strip() if title_tag else ""
            
            # æå–ä¸»è¦å†…å®¹
            main_content = self._quick_extract_content(soup)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            max_length = settings.web_content_max_length
            if len(main_content) > max_length:
                main_content = main_content[:max_length] + "..."
            
            end_time = asyncio.get_event_loop().time()
            duration = end_time - start_time
            
            self.logger.info(f"[çœŸå¹¶è¡Œ] å®ŒæˆURL {index}: {search_result.url} (è€—æ—¶: {duration:.2f}s)")
            
            # æ„é€ è¿”å›ç»“æœï¼ˆç®€åŒ–ç‰ˆWebContentDataï¼‰
            from app.models.chat import WebContentData
            return WebContentData(
                url=search_result.url,
                title=title,
                content=main_content,
                status="success"
            )
            
        except Exception as e:
            end_time = asyncio.get_event_loop().time()
            duration = end_time - start_time
            self.logger.error(f"[çœŸå¹¶è¡Œ] å¤±è´¥URL {index}: {search_result.url} (è€—æ—¶: {duration:.2f}s) - {e}")
            return None
    
    def _quick_extract_content(self, soup):
        """å¿«é€Ÿæå–é¡µé¢ä¸»è¦å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "iframe", "noscript"]):
            element.decompose()
        
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        main_selectors = ['main', 'article', '.content', '.main-content']
        main_element = None
        
        for selector in main_selectors:
            elements = soup.select(selector, limit=1)
            if elements:
                main_element = elements[0]
                break
        
        if not main_element:
            main_element = soup.find('body')
        
        if not main_element:
            return ""
        
        # æå–æ–‡æœ¬
        text_content = main_element.get_text(separator='\n', strip=True)
        
        # ç®€å•æ¸…ç†
        lines = text_content.split('\n')
        cleaned_lines = [line.strip() for line in lines if line.strip() and len(line.strip()) > 10]
        return '\n\n'.join(cleaned_lines[:50])  # é™åˆ¶è¡Œæ•°